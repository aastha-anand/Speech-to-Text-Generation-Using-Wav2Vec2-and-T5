# -*- coding: utf-8 -*-
"""Untitled16 (2) (1) (5).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15gyn1f2YoVmehS3ZM0SGLnrd6o9DuKSV
"""

!mkdir ~/.kaggle

!pip install kaggle

!cp kaggle.json ~/./root/.kaggle/

!kaggle datasets download -d shahrukhkhan/wikisql

!unzip wikisql.zip

import numpy as np
import pandas as pd

df = pd.read_csv("train.csv")

pd.read_csv("train.csv")

df_val= pd.read_csv("validation.csv")

import pandas as pd
import os
import re
train_csv_path = "train.csv"  # Update this path
train_df = pd.read_csv(train_csv_path)

audio_dir = "MyAudioFiless/"

audio_files = [f for f in os.listdir(audio_dir) if f.endswith(".wav")]

# Extract numbers from filenames and sort numerically
def extract_number(filename):
    match = re.search(r"question_(\d+)\.wav", filename)
    return int(match.group(1)) if match else float("inf")

audio_files.sort(key=extract_number)

# Ensure we have the same number of audio files as questions
num_questions = min(len(train_df), len(audio_files))

# Assign audio paths sequentially
train_df = train_df.iloc[:num_questions].copy()
train_df["audio_path"] = [os.path.join(audio_dir, f) for f in audio_files[:num_questions]]

df_audio_train = train_df

# Save the new dataset (optional)
df_audio_train.to_csv("df_audio_train.csv", index=False)

print("Audio mappings completed. df_audio_train created with", len(df_audio_train), "entries.")

pd.read_csv("df_audio_train.csv")

!pip install transformers

import os

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, DataCollatorForSeq2Seq, TFAutoModelForSeq2SeqLM

'''
# Load the dataframe
df = pd.read_csv("train.csv")

gpus = tf.config.list_physical_devices('GPU')
if len(gpus) == 0:
  print('No GPU found. Using CPU.')
  device = '/CPU:0'
  gpu_available = False
elif len(gpus) == 1:
    print('Only one GPU (0) found, using CPU. Modify code if you want to use GPU 0')
    device = '/CPU:0'
    gpu_available = False
else:
    try:
       with tf.device('/device:GPU:1'):
            print('Trying to use GPU 1')
            device = '/device:GPU:1'
            gpu_available = True
    except:
        print('GPU 1 not available, using CPU')
        device = '/CPU:0'
        gpu_available = False'''

!nvidia-smi

df = pd.read_csv("train.csv")

import os
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AutoModelForCausalLM

from sklearn.model_selection import train_test_split
import pandas as pd

!pip install transformers

import torch
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model, T5Tokenizer, T5ForConditionalGeneration

class SpeechToSQLModel(torch.nn.Module):
    def __init__(self):
        super(SpeechToSQLModel, self).__init__()
        self.wav2vec2 = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")
        self.t5 = T5ForConditionalGeneration.from_pretrained("t5-small")
        self.projection = torch.nn.Linear(768, 512)

    def forward(self, input_values, decoder_input_ids):
        audio_features = self.wav2vec2(input_values).last_hidden_state
        projected_features = self.projection(audio_features)
        outputs = self.t5(encoder_outputs=(projected_features,), decoder_input_ids=decoder_input_ids)
        return outputs

model = SpeechToSQLModel()
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from sklearn.model_selection import train_test_split

model_checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["question"], df["sql"], test_size=0.2, random_state=42
)

def preprocess_data(questions, queries):
    inputs = ["translate natural language to SQL: " + q for q in questions]
    targets = list(queries)

    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=512, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_data = preprocess_data(train_texts, train_labels)
val_data = preprocess_data(val_texts, val_labels)

class SQLDataset(torch.utils.data.Dataset):
    def __init__(self, data):
        self.input_ids = torch.tensor(data["input_ids"], dtype=torch.long)
        self.attention_mask = torch.tensor(data["attention_mask"], dtype=torch.long)
        self.labels = torch.tensor(data["labels"], dtype=torch.long)

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_mask[idx],
            "labels": self.labels[idx],
        }

train_dataset = SQLDataset(train_data)
val_dataset = SQLDataset(val_data)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="pt")

batch_size = 16
train_dataloader = torch.utils.data.DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator
)
val_dataloader = torch.utils.data.DataLoader(
    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
import nltk
from jiwer import wer, cer
from rouge_score import rouge_scorer

nltk.download("punkt")
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Split dataset
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["question"], df["sql"], test_size=0.2, random_state=42
)

# Preprocessing function
def preprocess_data(questions, queries):
    inputs = ["translate natural language to SQL: " + q for q in questions]
    targets = list(queries)

    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=512, truncation=True, padding="max_length")

    return {
        "input_ids": model_inputs["input_ids"],
        "attention_mask": model_inputs["attention_mask"],
        "labels": labels["input_ids"],
    }

train_data = preprocess_data(train_texts, train_labels)
val_data = preprocess_data(val_texts, val_labels)

# Custom PyTorch Dataset
class SQLDataset(Dataset):
    def __init__(self, data):
        self.input_ids = torch.tensor(data["input_ids"], dtype=torch.long)
        self.attention_mask = torch.tensor(data["attention_mask"], dtype=torch.long)
        self.labels = torch.tensor(data["labels"], dtype=torch.long)

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_mask[idx],
            "labels": self.labels[idx],
        }

# Create dataset instances
train_dataset = SQLDataset(train_data)
val_dataset = SQLDataset(val_data)

# Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="pt")

# DataLoader
batch_size = 8
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)

# Hyperparameters
learning_rate = 2e-5
epochs = 10

# Optimizer and Loss function
optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # Ignore padding tokens in loss

# Training loop
for epoch in range(epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")

    model.train()
    epoch_loss = 0.0
    for step, batch in enumerate(train_dataloader):
        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch to GPU

        optimizer.zero_grad()
        outputs = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            labels=batch["labels"],
        )
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        if (step + 1) % 10 == 0:
            print(f"Step: {step + 1}/{len(train_dataloader)}, Loss: {loss.item():.4f}")

    print(f"Average Epoch Training Loss: {epoch_loss / len(train_dataloader):.4f}")

# Evaluation after training
model.eval()
predictions, references = [], []
scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
rouge_scores = []
with torch.no_grad():
    for step, batch in enumerate(val_dataloader):
        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch to GPU

        outputs = model.generate(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            max_length=512
        )
        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        decoded_labels = tokenizer.batch_decode(batch["labels"], skip_special_tokens=True)

        predictions.extend(decoded_preds)
        references.extend(decoded_labels)

        for pred, ref in zip(decoded_preds, decoded_labels):
            rouge_scores.append(scorer.score(ref, pred)["rougeL"].fmeasure)

bleu_scores = [sentence_bleu([ref.split()], pred.split(), smoothing_function=SmoothingFunction().method1) for ref, pred in zip(references, predictions)]
avg_bleu = sum(bleu_scores) / len(bleu_scores)
print(f"BLEU Score: {avg_bleu:.4f}")

word_error = wer(references, predictions)
char_error = cer(references, predictions)
print(f"WER: {word_error:.4f}, CER: {char_error:.4f}")

avg_rouge = sum(rouge_scores) / len(rouge_scores)
print(f"ROUGE-L Score: {avg_rouge:.4f}")

sample_output = model.generate(batch["input_ids"][:1])
print("Raw Model Output (Token IDs):", sample_output[0].tolist())

save_path = "./t5_sql_translation"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"Model and tokenizer saved at {save_path}")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

load_path = "./t5_sql_translation"
tokenizer = AutoTokenizer.from_pretrained(load_path)
model = AutoModelForSeq2SeqLM.from_pretrained(load_path)

# Move to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print("Model and tokenizer loaded successfully!")

import shutil

save_path = "./t5_sql_translation"
zip_filename = "t5_sql_translation.zip"

shutil.make_archive(save_path, 'zip', save_path)

print(f"Model and tokenizer zipped as {zip_filename}")

!unzip MyAudioFiless-20250211T094901Z-001.zip

import torch
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model, T5Tokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import numpy as np
import pandas as pd
import torchaudio

# Dataset class tailored to your CSV
class SpeechSQLDataset(Dataset):
    def __init__(self, csv_file, feature_extractor, tokenizer, max_sql_length=128):
        self.dataframe = pd.read_csv(csv_file)
        self.feature_extractor = feature_extractor
        self.tokenizer = tokenizer
        self.max_sql_length = max_sql_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        audio_path = self.dataframe['audio_path'][idx]
        sql_query = self.dataframe['sql'][idx]

        # 1. Load and preprocess audio
        try:
            audio_array, sampling_rate = torchaudio.load(audio_path)
            audio_array = audio_array.squeeze().numpy()
            if sampling_rate != 16000:
                resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)
                audio_array = resampler(torch.tensor(audio_array)).numpy()
                sampling_rate = 16000

        except Exception as e:
            print(f"Error loading audio file: {audio_path}. Using random data. Error: {e}")
            audio_array = np.random.randn(16000)
            sampling_rate = 16000

        input_features = self.feature_extractor(audio_array, sampling_rate=sampling_rate, return_tensors="pt", padding="longest", truncation=True)
        input_values = input_features.input_values
        attention_mask = input_features.attention_mask

        # 2. Tokenize SQL query
        labels = self.tokenizer(sql_query, return_tensors="pt", padding="longest", truncation=True, max_length=self.max_sql_length).input_ids
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            "input_values": input_values.squeeze(),
            "attention_mask": attention_mask.squeeze(),
            "labels": labels.squeeze()
        }

def create_speech_to_sql_model():
    class SpeechToSQLModel(torch.nn.Module):
        def __init__(self):
            super(SpeechToSQLModel, self).__init__()
            self.wav2vec2 = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")
            self.t5 = T5ForConditionalGeneration.from_pretrained("t5-small")
            self.projection = torch.nn.Linear(self.wav2vec2.config.hidden_size, self.t5.config.hidden_size)

        def forward(self, input_values, attention_mask, labels=None):
            audio_features = self.wav2vec2(input_values, attention_mask=attention_mask).last_hidden_state
            projected_features = self.projection(audio_features)
            outputs = self.t5(encoder_outputs=(projected_features,), decoder_input_ids=None, labels=labels)
            return outputs

    return SpeechToSQLModel()

def train_speech_to_sql(model, train_dataloader, val_dataloader, optimizer, scheduler, device, num_epochs=3):
    model.to(device)
    model.train()

    for epoch in range(num_epochs):
        total_loss = 0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")
        for batch in progress_bar:
            input_values = batch["input_values"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            outputs = model(input_values, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            scheduler.step()

            total_loss += loss.item()
            progress_bar.set_postfix({"loss": total_loss / (progress_bar.n + 1e-5)})

        avg_loss = total_loss / len(train_dataloader)
        print(f"Epoch {epoch+1} - Average Training Loss: {avg_loss:.4f}")

def evaluate_speech_to_sql(model, dataloader, device):
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc="Evaluating")
        for batch in progress_bar:
            input_values = batch["input_values"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_values, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_val_loss += loss.item()
            progress_bar.set_postfix({"val_loss": total_val_loss / (progress_bar.n + 1e-5)})

    avg_val_loss = total_val_loss / len(dataloader)
    model.train()
    return avg_val_loss

feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# 2. Create Dataset and DataLoaders (moved inside if __name__ == "__main__":)
csv_file_path = "df_audio_train.csv"  # Path to your CSV file
train_dataset = SpeechSQLDataset(csv_file_path, feature_extractor, tokenizer)
val_dataset = SpeechSQLDataset(csv_file_path, feature_extractor, tokenizer)

train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=2)


# 3. Create Model
model = create_speech_to_sql_model()

# 4. Optimizer and Scheduler
optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = len(train_dataloader) * 3
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * num_training_steps), num_training_steps=num_training_steps)

# 5. Device
device = "cuda" if torch.cuda.is_available() else "cpu"

# 6. Train the Model
train_speech_to_sql(model, train_dataloader, val_dataloader, optimizer, scheduler, device, num_epochs=3)

print("Training finished!")

# 7. Save the trained model (Optional)
# torch.save(model.state_dict(), "speech_to_sql_model.pth")

from transformers import T5Tokenizer

tokenizer_test = T5Tokenizer.from_pretrained("t5-small")
test_sql_query = "SELECT * FROM table1"

try:
    test_tokenized = tokenizer_test(
        test_sql_query,
        return_tensors="pt",
        padding="longest",
        truncation=True,
        max_length=128
    )
    print("Tokenizer Test Successful:")
    print(test_tokenized)
except ValueError as e:
    print("Tokenizer Test Failed with ValueError:")
    print(e)
except Exception as e:
    print(f"Tokenizer Test Failed with error: {e}")

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

# Load audio file (change 'your_audio_file.wav' to your file)
audio_path = "MyAudioFiless/question_1.wav"
y, sr = librosa.load(audio_path, sr=None)

# Create a figure with two subplots
fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 8))

# Plot waveform
librosa.display.waveshow(y, sr=sr, ax=ax[0], color='b')
ax[0].set_title("Waveform")
ax[0].set_xlabel("Time (s)")
ax[0].set_ylabel("Amplitude")

# Compute and plot spectrogram
D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
librosa.display.specshow(D, sr=sr, x_axis="time", y_axis="log", ax=ax[1])
ax[1].set_title("Spectrogram")
ax[1].set_xlabel("Time (s)")
ax[1].set_ylabel("Frequency (Hz)")
fig.colorbar(librosa.display.specshow(D, sr=sr, x_axis="time", y_axis="log", ax=ax[1]), ax=ax[1], format="%+2.0f dB")

# Show plots
plt.tight_layout()
plt.show()
plt.savefig("spectogram.png")

import pandas as pd
import io

csv_data = """question,sql,audio_path
Tell me what the notes are for South Australia ,SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA,MyAudioFiless/question_1.wav
What is the current series where the new series began in June 2011?,SELECT Current series FROM table WHERE Notes = New series began in June 2011,MyAudioFiless/question_2.wav
What is the format for South Australia?,SELECT Format FROM table WHERE State/territory = South Australia,MyAudioFiless/question_3.wav
Name the background colour for the Australian Capital Territory,SELECT Text/background colour FROM table WHERE State/territory = Australian Capital Territory,MyAudioFiless/question_4.wav
how many times is the fuel propulsion is cng?,SELECT COUNT Fleet Series (Quantity) FROM table WHERE Fuel Propulsion = CNG,MyAudioFiless/question_5.wav
what is the fuel propulsion where the fleet series (quantity) is 310-329 (20)?,SELECT Fuel Propulsion FROM table WHERE Fleet Series (Quantity) = 310-329 (20),MyAudioFiless/question_6.wav
who is the manufacturer for the order year 1998?,SELECT Manufacturer FROM table WHERE Order Year = 1998,MyAudioFiless/question_7.wav
how many times is the model ge40lfr?,SELECT COUNT Manufacturer FROM table WHERE Model = GE40LFR,MyAudioFiless/question_8.wav
how many times is the fleet series (quantity) is 468-473 (6)?,SELECT COUNT Order Year FROM table WHERE Fleet Series (Quantity) = 468-473 (6),MyAudioFiless/question_9.wav
what is the powertrain (engine/transmission) when the order year is 2000?,SELECT Powertrain (Engine/Transmission) FROM table WHERE Order Year = 2000,MyAudioFiless/question_10.wav
"""

df_audio_train = pd.read_csv(io.StringIO(csv_data))

question_word_counts = []
sql_token_counts = []

for index, row in df_audio_train.iterrows():
    question = row['question']
    sql = row['sql']

    question_words = question.split()
    question_word_counts.append(len(question_words))

    sql_tokens = sql.split()
    sql_token_counts.append(len(sql_tokens))

avg_question_words = sum(question_word_counts) / len(question_word_counts)
range_question_words = (min(question_word_counts), max(question_word_counts))

avg_sql_tokens = sum(sql_token_counts) / len(sql_token_counts)
range_sql_tokens = (min(sql_token_counts), max(sql_token_counts))

output_string = f"[{avg_question_words:.2f}/{range_question_words[0]}-{range_question_words[1]}] words per utterance and an average SQL query length of [{avg_sql_tokens:.2f}/{range_sql_tokens[0]}-{range_sql_tokens[1]}] tokens"

print(output_string)

import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
import re

# Load your CSV file
df_audio_train = pd.read_csv('df_audio_train.csv') # Replace 'df_audio_train.csv' with your actual file path

table_names = []
for sql_query in df_audio_train['sql']:
    match = re.search(r'FROM\s+([\w\s]+)(?:\s+WHERE|$)', sql_query, re.IGNORECASE) # Regex to find table name
    if match:
        table_name = match.group(1).strip()
        table_names.append(table_name)

table_counts = Counter(table_names)

tables = list(table_counts.keys())
counts = list(table_counts.values())

plt.figure(figsize=(8, 6))
plt.bar(tables, counts, edgecolor='black')
plt.title('Frequency of Tables Referenced in SQL Queries')
plt.xlabel('Table Names')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', alpha=0.75)
plt.tight_layout()
plt.show()